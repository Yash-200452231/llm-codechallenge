{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPgFu0ELUnqJQ1E1DirysH5",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Yash-200452231/llm-codechallenge/blob/main/Edgecom_Energy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Coding Challenge Edgecom Energy"
      ],
      "metadata": {
        "id": "c02VO1eQ1VEh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q transformers[torch] datasets"
      ],
      "metadata": {
        "id": "i6N-1YHFD1J6"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Sample Dataset"
      ],
      "metadata": {
        "id": "NK61vQeLDKm1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the dataset\n",
        "data = {\n",
        "    'Text_Sequence': [\n",
        "        \"add a constraint of 4X + 5Y <= 40 in our model.\",\n",
        "        \"please include the constraint 7X - 3Y == 15 in our Pyomo model.\",\n",
        "        \"could you add 5X + 8Y >= 60 as a new constraint?\",\n",
        "        \"I need a constraint: 9X + 2Y < 25 added to the model.\",\n",
        "        \"add 3X + 4Y > 18 as a constraint.\",\n",
        "        \"please add a constraint of 6X + 7Y <= 50.\",\n",
        "        \"we need to include 8X - 2Y >= 10 in the constraints.\",\n",
        "        \"let's add 2X + 9Y == 35 as another constraint.\",\n",
        "        \"can you include the constraint 4X - 6Y < 20 in the model?\",\n",
        "        \"add a new constraint: 3X + 5Y > 28.\",\n",
        "        \"let's include a constraint of 7X + 3Y <= 45.\",\n",
        "        \"add 5X - 2Y >= 20 to the constraints.\",\n",
        "        \"I need the constraint 6X + 4Y == 50 added.\",\n",
        "        \"please add 2X - 3Y < 15 as a constraint.\",\n",
        "        \"add the constraint 9X + 7Y > 70 to the model.\",\n",
        "        \"include 4X + 6Y <= 60 as a new constraint.\",\n",
        "        \"can you add 3X - 5Y >= 25 in the model constraints?\",\n",
        "        \"add a constraint 5X + 4Y == 40 to the model.\",\n",
        "        \"please include 6X - 2Y < 22 in the constraints.\",\n",
        "        \"add another constraint: 8X + 5Y > 55.\"\n",
        "    ],\n",
        "    'Label': [\n",
        "        \"model.constraint3 = Constraint(expr=4 * model.x + 5 * model.y <= 40)\",\n",
        "        \"model.constraint3 = Constraint(expr=7 * model.x - 3 * model.y == 15)\",\n",
        "        \"model.constraint3 = Constraint(expr=5 * model.x + 8 * model.y >= 60)\",\n",
        "        \"model.constraint3 = Constraint(expr=9 * model.x + 2 * model.y < 25)\",\n",
        "        \"model.constraint3 = Constraint(expr=3 * model.x + 4 * model.y > 18)\",\n",
        "        \"model.constraint3 = Constraint(expr=6 * model.x + 7 * model.y <= 50)\",\n",
        "        \"model.constraint3 = Constraint(expr=8 * model.x - 2 * model.y >= 10)\",\n",
        "        \"model.constraint3 = Constraint(expr=2 * model.x + 9 * model.y == 35)\",\n",
        "        \"model.constraint3 = Constraint(expr=4 * model.x - 6 * model.y < 20)\",\n",
        "        \"model.constraint3 = Constraint(expr=3 * model.x + 5 * model.y > 28)\",\n",
        "        \"model.constraint3 = Constraint(expr=7 * model.x + 3 * model.y <= 45)\",\n",
        "        \"model.constraint3 = Constraint(expr=5 * model.x - 2 * model.y >= 20)\",\n",
        "        \"model.constraint3 = Constraint(expr=6 * model.x + 4 * model.y == 50)\",\n",
        "        \"model.constraint3 = Constraint(expr=2 * model.x - 3 * model.y < 15)\",\n",
        "        \"model.constraint3 = Constraint(expr=9 * model.x + 7 * model.y > 70)\",\n",
        "        \"model.constraint3 = Constraint(expr=4 * model.x + 6 * model.y <= 60)\",\n",
        "        \"model.constraint3 = Constraint(expr=3 * model.x - 5 * model.y >= 25)\",\n",
        "        \"model.constraint3 = Constraint(expr=5 * model.x + 4 * model.y == 40)\",\n",
        "        \"model.constraint3 = Constraint(expr=6 * model.x - 2 * model.y < 22)\",\n",
        "        \"model.constraint3 = Constraint(expr=8 * model.x + 5 * model.y > 55)\"\n",
        "    ]\n",
        "}\n",
        "\n",
        "# Create a DataFrame\n",
        "#df = pd.DataFrame(data)\n",
        "\n",
        "# Save to CSV\n",
        "#df.to_csv('dummy_constraints_dataset.csv', index=False)\n",
        "\n",
        "# Display the DataFrame\n",
        "#print(df)\n"
      ],
      "metadata": {
        "id": "jOOuYFKgNd1W"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(data['Label']), len(data['Text_Sequence'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VLud5ycD2P3M",
        "outputId": "1446bebb-119d-4726-c82d-2ccfde1afa88"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(20, 20)"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Basic Training script for T5"
      ],
      "metadata": {
        "id": "QoQYMRUkDJP9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration, Trainer, TrainingArguments\n",
        "import pandas as pd\n",
        "import torch\n",
        "\n",
        "\n",
        "# Create DataFrame\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Initialize the tokenizer and model\n",
        "model_name = 'google/flan-t5-small'\n",
        "tokenizer = T5Tokenizer.from_pretrained(model_name)\n",
        "model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
        "\n",
        "# Prepare the dataset\n",
        "train_texts = df['Text_Sequence'].tolist()\n",
        "train_labels = df['Label'].tolist()\n",
        "\n",
        "# Tokenize the dataset\n",
        "train_encodings = tokenizer(train_texts, truncation=True, padding='max_length', max_length=512, return_tensors='pt')\n",
        "train_labels_encodings = tokenizer(train_labels, truncation=True, padding='max_length', max_length=512, return_tensors='pt')\n",
        "\n",
        "# Define Dataset Class\n",
        "class ConstraintDataset(torch.utils.data.Dataset):\n",
        "    def __init__(self, encodings, labels):\n",
        "        self.encodings = encodings\n",
        "        self.labels = labels\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        item = {key: val[idx] for key, val in self.encodings.items()}\n",
        "        item['labels'] = self.labels['input_ids'][idx]\n",
        "        return item\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.encodings.input_ids)\n",
        "\n",
        "train_dataset = ConstraintDataset(train_encodings, train_labels_encodings)\n",
        "\n",
        "# Define training arguments\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=3,\n",
        "    per_device_train_batch_size=4,\n",
        "    save_steps=10_000,\n",
        "    save_total_limit=2,\n",
        ")\n",
        "\n",
        "# Initialize the Trainer\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        ")\n",
        "\n",
        "# Fine-tune the model\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "id": "YXwRDN606kPn",
        "outputId": "0dbfe1f9-d804-4f68-efe6-e3c777a634ec"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n",
            "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='15' max='15' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [15/15 05:15, Epoch 3/3]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=15, training_loss=39.9119384765625, metrics={'train_runtime': 339.4107, 'train_samples_per_second': 0.177, 'train_steps_per_second': 0.044, 'total_flos': 11153430282240.0, 'train_loss': 39.9119384765625, 'epoch': 3.0})"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
        "\n",
        "# Load the fine-tuned model and tokenizer\n",
        "#model_path = './results'  # Path to the fine-tuned model directory\n",
        "#tokenizer = T5Tokenizer.from_pretrained(model_path)\n",
        "#model = T5ForConditionalGeneration.from_pretrained(model_path)\n",
        "\n",
        "# Function to generate constraint code from input text\n",
        "def generate_constraint_code(input_text):\n",
        "    # Tokenize the input text\n",
        "    input_ids = tokenizer(input_text, return_tensors='pt').input_ids\n",
        "\n",
        "    # Generate the output using the model\n",
        "    outputs = model.generate(input_ids)\n",
        "\n",
        "    # Decode the generated output\n",
        "    generated_code = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return generated_code\n",
        "\n",
        "# Example input text sequence\n",
        "input_text = \"add a constraint of 2X + 6Y > 30 in our model.\"\n",
        "\n",
        "# Generate and print the output\n",
        "generated_code = generate_constraint_code(input_text)\n",
        "print(f\"Input: {input_text}\")\n",
        "print(f\"Generated Code: {generated_code}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHaR15Gc6p9V",
        "outputId": "57b21136-d9b5-4cbe-e39c-6f0be6ef30e8"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/generation/utils.py:1168: UserWarning: Using the model-agnostic default `max_length` (=20) to control the generation length. We recommend setting `max_new_tokens` to control the maximum length of the generation.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input: add a constraint of 2X + 6Y > 30 in our model.\n",
            "Generated Code: Add a constraint of 2X + 6Y > 30 in our model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PEFT(LoRA) Training with quantization"
      ],
      "metadata": {
        "id": "C9iUJyhyDTE1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()"
      ],
      "metadata": {
        "id": "Zdw_pkZgIwZL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from peft import LoraConfig\n",
        "\n",
        "# Load the 7b llama model\n",
        "model_id = \"meta-llama/Llama-2-7b-hf\"\n",
        "\n",
        "quantization_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_quant_type=\"nf4\"\n",
        ")\n",
        "\n",
        "# Load model\n",
        "model = AutoModelForCausalLM.from_pretrained(model_id, quantization_config=quantization_config)\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_id, trust_remote_code=True)\n",
        "# Set it to a new token to correctly attend to EOS tokens.\n",
        "tokenizer.add_special_tokens({'pad_token': '<PAD>'})"
      ],
      "metadata": {
        "id": "WTRLhGSMDSVS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    target_modules=[\"q_proj\", \"o_proj\", \"k_proj\", \"v_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\",\n",
        ")\n",
        "\n",
        "model.add_adapter(lora_config)"
      ],
      "metadata": {
        "id": "Yb0Q6gBwIKf1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import TrainingArguments\n",
        "\n",
        "YOUR_HF_USERNAME = \"\"\n",
        "\n",
        "output_dir = f\"/content/llama-7b-qlora-ultrachat\"\n",
        "per_device_train_batch_size = 4\n",
        "gradient_accumulation_steps = 4\n",
        "optim = \"paged_adamw_32bit\"\n",
        "save_steps = 10\n",
        "logging_steps = 10\n",
        "learning_rate = 2e-4\n",
        "max_grad_norm = 0.3\n",
        "max_steps = 1000\n",
        "warmup_ratio = 0.03\n",
        "lr_scheduler_type = \"constant\"\n",
        "\n",
        "training_arguments = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    per_device_train_batch_size=per_device_train_batch_size,\n",
        "    gradient_accumulation_steps=gradient_accumulation_steps,\n",
        "    optim=optim,\n",
        "    save_steps=save_steps,\n",
        "    logging_steps=logging_steps,\n",
        "    learning_rate=learning_rate,\n",
        "    max_grad_norm=max_grad_norm,\n",
        "    max_steps=max_steps,\n",
        "    warmup_ratio=warmup_ratio,\n",
        "    lr_scheduler_type=lr_scheduler_type,\n",
        "    gradient_checkpointing=True,\n",
        "    push_to_hub=True,\n",
        ")"
      ],
      "metadata": {
        "id": "NxeN3N-XIPoA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from trl import SFTTrainer\n",
        "\n",
        "def formatting_func(example):\n",
        "    text = f\"### USER: {example['data'][0]}\\n### CODE: {example['data'][1]}\"\n",
        "    return text"
      ],
      "metadata": {
        "id": "QCLXq6hFIfoK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = SFTTrainer(\n",
        "    model=model,\n",
        "    args=training_arguments,\n",
        "    train_dataset=train_dataset,\n",
        "    packing=True,\n",
        "    dataset_text_field=\"id\",\n",
        "    tokenizer=tokenizer,\n",
        "    max_seq_length=512,\n",
        "    formatting_func=formatting_func,\n",
        ")"
      ],
      "metadata": {
        "id": "TcvNcTZxIpl3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "VUVd2g2IIqlq"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}